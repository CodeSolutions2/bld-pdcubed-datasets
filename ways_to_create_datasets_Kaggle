https://www.tensorflow.org/datasets/gcs

# --------------------------------------------------
# Ways to create Datasets
# --------------------------------------------------

!pip install Wikipedia-API

# --------------------------------------------------

import numpy as np
import pandas as pd

# https://pypi.org/project/Wikipedia-API/
import wikipediaapi

import regex

import tensorflow as tf
import tensorflow_datasets as tfds

# --------------------------------------------------

# Subfunctions

# --------------------------------------------------

def get_next_break_char_index(text_temp):
    temp = [text_temp.find('\n'), text_temp.find('.')]
    out = np.sort(temp)
    # print('out: ', out)
    
    foundvals = [i for i in out if i > -1]
    if any(foundvals):
        ender = min(foundvals)
    else:
        ender = len(text_temp)  # no values found
    # print('ender: ', ender)
    
    return ender

# --------------------------------------------------

# Transform plain character text to a sentence array, where each sentence is in a nested array.
def text_2_sentences(text):
    flag = 0
    ender = -1
    ender_b4 = ender+1
    text_temp = text # a temporary variable to not copy over text
    
    sen = []
    while ender_b4 < len(text_temp):
        ender_b4 = ender+1
        # print('ender_b4: ', ender_b4)
        
        ender = get_next_break_char_index(text_temp[ender_b4::]) + ender_b4
        # print('ender: ', ender)
        
        sen.append(text_temp[ender_b4:ender])
        # print('sen: ', sen)
    
    # Remove empty nested arrays 
    sen = [i for i in sen if any(i)]
    # print('sen: ', sen)

    return sen

# --------------------------------------------------

# Dataset 0: for BERTopic classification (inverse zero-shot)

## Determine if sentences are: breakfast, lunch, dinner

# --------------------------------------------------

wiki_wiki = wikipediaapi.Wikipedia('MyProjectName', 'en', extract_format=wikipediaapi.ExtractFormat.WIKI)
p_wiki = wiki_wiki.page("Breakfast")
breafast_text = p_wiki.text
print('breafast_text: ', breafast_text)

# --------------------------------------------------

wiki_wiki = wikipediaapi.Wikipedia('MyProjectName', 'en', extract_format=wikipediaapi.ExtractFormat.WIKI)
p_wiki = wiki_wiki.page("Lunch")
lunch_text = p_wiki.text
print('lunch_text: ', lunch_text)

# --------------------------------------------------

wiki_wiki = wikipediaapi.Wikipedia('MyProjectName', 'en', extract_format=wikipediaapi.ExtractFormat.WIKI)

p_wiki = wiki_wiki.page("Dinner")
dinner_text = p_wiki.text
print('dinner_text: ', dinner_text)

# --------------------------------------------------

breafast_sentences = text_2_sentences(breafast_text)
lunch_sentences = text_2_sentences(lunch_text)
dinner_sentences = text_2_sentences(dinner_text)


# Concatenate sentences and labels
sentences = breafast_sentences + lunch_sentences + dinner_sentences

labels = ['breafast']*len(breafast_sentences) + ['lunch']*len(lunch_sentences) + ['dinner']*len(dinner_sentences)

# --------------------------------------------------

print('len(sentences): ', len(sentences), ', len(labels): ', len(labels))

# --------------------------------------------------

# --------------------------------------------------
# Lightly clean data
# --------------------------------------------------

X = []
Y = []

for i in range(len(sentences)):
    
    sen_str = sentences[i]
    y_str = labels[i]
    
    # ----------------------------
    # Clean/pre-process the sentence
    # ----------------------------
    # [Step 0] Make sentences lowercase
    sen_str = sen_str.lower()
    
    # [Step 1] Remove parentheses and text in between parentheses, so that phrases are gramatically correct
    sen_str = regex.sub("(\(.*?\))", "", sen_str)
    sen_str = regex.sub("(\{.*?\})", "", sen_str)
    sen_str = regex.sub("(\[.*?\])", "", sen_str)
    
    # [Step 2] Remove long undesireable characters repeatively, matching characters in the string
    # These words should be unique words, such that parts of other words are not be modified 
    undesireable_chars = ['</p>', '<a', 'id=', "href=", 'title=', 'class=', '</a>', 
                          '</sup>', '<p>', '</b>', '<sup']
    for word in undesireable_chars:
        sen_str = regex.sub(r"\b" + word + r"\b", '', sen_str)
    
    # [Step 3] Finally remove all multiple spaces, and replace with a single space
    sen_str = regex.sub(r'\s+', ' ', sen_str)
    
    # ----------------------------
        
    # [1] Remove sentences with less than 10 words. Narrow the sentences down to realistic sentences.
    if (len(sen_str.split()) > 10) & (len(y_str) > 0):
        X.append(sen_str)
        Y.append(y_str)

# --------------------------------------------------

## Put in Pandas DataFrame

df = pd.DataFrame(X, columns=['X'])
df = pd.concat([df, pd.Series(Y)], axis=1)
df.columns = ['X', 'Y']
df

# --------------------------------------------------

save_dataset_name = 'BLD_dataset.csv'

# Save csv
df.to_csv(save_dataset_name, index=False)


# -------------------------------------------------
# Put Datasets on GCP Storage: https://console.cloud.google.com/
# -------------------------------------------------
# Determine what project to put the files
gcloud projects list

PROJECT_ID="text-pr0cessing"

# Set the project
gcloud config set project $PROJECT_ID

# ---------------------------------------------

# Create Storage Bucket
BUCKET_NAME="bld-dataset"

LOCATION="europe-west9"
gcloud storage buckets create gs://$BUCKET_NAME --project=$PROJECT_ID --default-storage-class=STANDARD --location=$LOCATION --uniform-bucket-level-access


// Dataset is on Kaggle
// Get service account permission to copy dataset to GCP Storage

// ---------
// On GCP: set up the service account key
// ---------
# List service accounts
gcloud iam service-accounts list

# There are no service accounts set up for text-pr0cessing, so set one up
export SERVICE_ACCOUNT_EMAIL=$(echo "j622amilah@gmail.com")
export SERVICE_ACCOUNT_ID=$(echo "GCP-login")
export SERVICE_ACCOUNT=$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com

# Create a custom service account
gcloud iam service-accounts create $SERVICE_ACCOUNT_ID --description="Creating a service account for GCP Storage" --display-name="GCP-login"

# Add the role to access Storage objects
gcloud projects add-iam-policy-binding $PROJECT_ID --member=serviceAccount:$SERVICE_ACCOUNT --role="roles/storage.objectAdmin"

# Create a service account key
# https://cloud.google.com/sdk/gcloud/reference/iam/service-accounts/keys/create
gcloud iam service-accounts keys create key.json --iam-account=$SERVICE_ACCOUNT

cat key.json

copy-paste text inside key.json

clear



// ---------
// On Kaggle
// ---------
%%bash --err null
cat > key.json <<EOF
{
  "type": "service_account",
  "project_id": "text-pr0cessing",
  "private_key_id": "8f7672ccfab3caccbe6d72a0f38e431aa9a56f46",
  "private_key": "-----BEGIN PRIVATE KEY-----\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCVSisSyHVzGSvs\nWyv7Qdz2UyXAvrGB8VyqMeK4tjSKTUwSN39vGCdJRK1eCNoKnwSobXwBpG1mDvFS\n2kd3XoXerDKpCXL0clpTCgPz7MEAGLMRy/qHgIBKhQEXYgrOy/KmbDoHtTLyAkVz\n08f0rGJzoF/DdxOrznfeibMVEnjIEXWsizfYKpbYnjMJ0W/QF8YRa4grTW43CKqY\nseI2/vDEP9MHJdniwMhl6vv5wnWtVPas6Y7zh1/9MOjfrQCgyNhrygCs6n074RRq\nOhdu7ixrYQNJJAMBmtqou300/SKI9M7vujuWpi8YT/JCC3aosRN/TpPySercFCho\nHYwTK2E3AgMBAAECggEAM9qgrnQBU5q1JnIeizW71bVYwiOaTVu0B4uvR+BFzJn/\np74uHbzifHEF1Mk3TwQf3o60yW8+7nmXOeOGekBTOF5QdGVLT/veQsSSTen4uZta\nq35RxToNlZSwJdqQjXbKkCK8X0wBZ40BabNwWxx7T9UvHhogs4bSHPJwSQvy9u6N\nut2rKtKt9/j39mo30CCz0dD3wYL6DsgXSElHqUekQJrY3xbDgERLuODtmPMGREUs\n5nasQcS7OccLn3EbkPf6kOvvDT4gOP+aWL1igGDXjcqLaldVltPA9MJwqpGBnkV3\n3VC8tuv66zg0HNP3mfhxzxABBUqG11zlzdh5HUUxjQKBgQDIc6tlP1bpDaN2ax7Q\n4OTzc3rVkOAQLuar+2VnhjfCIASfn6GVYDKoSQcD5x+lfOjR4QTKW3MXOGzCkDXu\n5lFHZm/TNFR7A9TvwwLqKVFFSPThyIzQu0NmkZkQrngIl5v23dT0gIra49PlPTSH\nqrvNI3TKZTUE42fvU/hGo0JexQKBgQC+qPzQjN+/jen5qjehlxXaek3GZYR3SZyD\no9ff+7Z+HtFl44I8l6e0ayIkqjQdnuStX3Jo5l0AXcWItrUEpfnoeLQHRwLjTsWn\njys4NK2WUZTplR4VI9cmQwMfDooMKcdkMst8BoR7RBku0ZFdu5miEJgXqbsAWDvL\nSUH3TtT/ywKBgBjUayixfdyEefkTL9AD65hUd6S4u2iFxsEK9Par44BCYxA6v9M3\nBU9fLGVJXdwr6tMEoolGZq07qg03u9aJ5QuJHtT0qHpMcTX53ASRdaW1cc/9/aNl\nbuXnpyQv2GKIFSOxOs3Glmyix/6fZEBh3717ZN4lRzoByUAWEKz/UBtZAoGANZ9R\ne1Yjl7xSEeXNv6S+gLPlK+dhWitZ2aw7CsPAj7gbLIzQFDqS8Csxbx5vieUma5xs\nkRQ5E+FLrSe1wZjssHVCLMnMBiIhdP7PKP+n0p8fQpKt40QxPpGaTyHHsaQOcB/Z\nGwi6OfDtJD3MpJAepXYFUKJr9azbJFF2PBfv0AECgYBdMeT6BBsCFBPy2vuet4TA\nC5pJD1NJfVjzL/A61sCl0EY7YLi9DfMNZVi6nNCV/GmaVpAiOwEH4iIj4E7hx+O/\nl4d8+1fSyjA4vE2oZ0GzG0wWOcXJzNOxBIjhJz7opIMXkLKu5Bm0N7aSeGd+NM2c\nkVOUxrpFjXVKSaM7NdJKiA==\n-----END PRIVATE KEY-----\n",
  "client_email": "GCP-login@text-pr0cessing.iam.gserviceaccount.com",
  "client_id": "105421319658417088530",
  "auth_uri": "https://accounts.google.com/o/oauth2/auth",
  "token_uri": "https://oauth2.googleapis.com/token",
  "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
  "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/x509/GCP-login%40text-pr0cessing.iam.gserviceaccount.com",
  "universe_domain": "googleapis.com"
}
EOF
# Copy-past service account key

!gcloud auth login --quiet --cred-file=key.json --force --project text-pr0cessing

!gcloud config set project text-pr0cessing --quiet

# Upload file to storage
!gcloud storage cp BLD_dataset.csv gs://bld-dataset

// ---------
// On GCP: verify that dataset is present
// ---------
gcloud storage ls --recursive gs://$BUCKET_NAME/**

# Make all objects in a bucket publicly readable
gcloud storage buckets add-iam-policy-binding gs://$BUCKET_NAME --member=allUsers --role=roles/storage.objectViewer


# ---------------------------------------------

# Check if the image can be viewed in the browser
https://storage.googleapis.com/bld-dataset/BLD_dataset.csv


# Use storage path to refer to image
gs://bld-dataset/BLD_dataset.csv



# --------------------------------------------------

## Tensorflow Datasets

## Way 0: with command line tool

!rm -rf BLD_dataset

# --------------------------------------------------

# Create a folder and python file called BLD_dataset/BLD_dataset.py
!tfds new BLD_dataset


# Dataset generated at /kaggle/working/BLD_dataset
# You can start searching `TODO(BLD_dataset)` to complete the implementation.
# Please check https://www.tensorflow.org/datasets/add_dataset for additional details.
# Open BLD_dataset.py and modify manually
# --------------------------------------------------

"""BLD_dataset dataset. Inside BLD_dataset_dataset_builder.py"""

import tensorflow_datasets as tfds


class Builder(tfds.core.GeneratorBasedBuilder):
  """DatasetBuilder for BLD_dataset dataset."""

  VERSION = tfds.core.Version('1.0.0')
  RELEASE_NOTES = {
      '1.0.0': 'Initial release.',
  }

  def _info(self) -> tfds.core.DatasetInfo:
    """Returns the dataset metadata."""
    # TODO(BLD_dataset): Specifies the tfds.core.DatasetInfo object
    return self.dataset_info_from_configs(
        features=tfds.features.FeaturesDict({
            # These are the features of your dataset like images, labels ...
            'image': tfds.features.Image(shape=(None, None, 3)),
            'label': tfds.features.ClassLabel(names=['no', 'yes']),
        }),
        # If there's a common (input, target) tuple from the
        # features, specify them here. They'll be used if
        # `as_supervised=True` in `builder.as_dataset`.
        supervised_keys=('image', 'label'),  # Set to `None` to disable
        homepage='https://dataset-homepage/',
    )

  def _split_generators(self, dl_manager: tfds.download.DownloadManager):
    """Returns SplitGenerators."""
    # TODO(BLD_dataset): Downloads the data and defines the splits
    path = dl_manager.download_and_extract('https://todo-data-url')

    # TODO(BLD_dataset): Returns the Dict[split names, Iterator[Key, Example]]
    return {
        'train': self._generate_examples(path / 'train_imgs'),
    }

  def _generate_examples(self, path):
    """Yields examples."""
    # TODO(BLD_dataset): Yields (key, example) tuples from the dataset
    for f in path.glob('*.jpeg'):
      yield 'key', {
          'image': f,
          'label': 'yes',
      }

# --------------------------------------------------

# One needs to be in the dataset directory, 
# Download and prepare the dataset for model usage
!tfds build /kaggle/working/BLD_dataset

# --------------------------------------------------


# --------------------------------------------------

## Way 1: TfDataBuilder

https://www.tensorflow.org/datasets/format_specific_dataset_builders

# --------------------------------------------------

def encode_labels(labels):
    
    unq_labels = list(set(labels))
    NUM_OF_CLASSES = len(unq_labels)
    
    # Assign a number to each unique label
    y_assignment = dict(zip(unq_labels, np.arange(NUM_OF_CLASSES)))
    print('y_assignment ', y_assignment)

    label_sequences = [y_assignment[i] for i in labels]
    
    return label_sequences, y_assignment

# --------------------------------------------------

label_sequences, y_assignment = encode_labels(Y)

# --------------------------------------------------

# Make test and train datasets
TRAINING_SPLIT = 0.7

train_size = int(TRAINING_SPLIT*len(X))
X_train = [X[i] for i in range(train_size)]
Y_train = [label_sequences[i] for i in range(train_size)]

X_test = [X[i] for i in range(train_size, len(X))]
Y_test = [label_sequences[i] for i in range(train_size, len(label_sequences))]

# --------------------------------------------------

Y_train = tf.convert_to_tensor(Y_train, dtype=tf.int32)
Y_test = tf.convert_to_tensor(Y_test, dtype=tf.int32)

# --------------------------------------------------

idx_train = tf.convert_to_tensor([i for i in range(len(X_train))], dtype=tf.int32)
idx_test = tf.convert_to_tensor([i for i in range(len(X_test))], dtype=tf.int32)

# --------------------------------------------------

data_dict_train = {"idx": idx_train,
            "label": Y_train,
            "sentence": X_train}

data_dict_test = {"idx": idx_test,
            "label": Y_test,
            "sentence": X_test}

# --------------------------------------------------

ds_train = tf.data.Dataset.from_tensor_slices(data_dict_train)
ds_test = tf.data.Dataset.from_tensor_slices(data_dict_test)

# --------------------------------------------------

!rm -rf BLD_dataset
!mkdir BLD_dataset

# --------------------------------------------------

# Make the Dataset locally

# Define the builder.
single_number_builder = tfds.dataset_builders.TfDataBuilder(
    name="BLD_dataset",
    config="single_number",
    version="1.0.0",
    data_dir="/kaggle/working/BLD_dataset",
    split_datasets={
        "train": ds_train,
        "test": ds_test,
    },
    features=tfds.features.FeaturesDict({
        'idx': tf.int32,
        'label': tfds.features.ClassLabel(num_classes=3),
        'sentence': tfds.features.Text(),
    }),
    description="Breakfast Lunch Dinner (BLD) dataset.",
    release_notes={
        "1.0.0": "Notes",
    }
)

# Make the builder store the data as a TFDS dataset.
single_number_builder.download_and_prepare()

# --------------------------------------------------

BLD_dataset

# --------------------------------------------------

# If I type BLD_dataset, it should print

# {'train': <_PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), 
#                     dtype=tf.int32, name=None), 'label': TensorSpec(shape=(None,), 
#                     dtype=tf.int64, name=None), 'sentence1': TensorSpec(shape=(None,), 
#                     dtype=tf.string, name=None), 'sentence2': TensorSpec(shape=(None,), 
#                     dtype=tf.string, name=None)}>,

#  'validation': <_PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'sentence1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'sentence2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>,
#  'test': <_PrefetchDataset element_spec={'idx': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'label': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'sentence1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'sentence2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}>}

# --------------------------------------------------

# If the TFDS was stored in a custom folder, then it can be loaded as follows:
custom_data_dir = "/kaggle/working/BLD_dataset"

ds_train = tfds.load("/BLD_dataset/single_number/1.0.0", 
                     split="train", 
                    data_dir=custom_data_dir
                    )

# --------------------------------------------------


info.features

# FeaturesDict({
#     'idx': int32,
#     'label': ClassLabel(shape=(), dtype=int64, num_classes=2),
#     'sentence1': Text(shape=(), dtype=string),
#     'sentence2': Text(shape=(), dtype=string),
# })

# --------------------------------------------------

# View the dataset
list(ds_train2.take(1).as_numpy_iterator())

example_batch = next(iter(ds_train2))
example_batch

example_batch.items()

# One should be able to view the data using dataset iteration
for key, value in example_batch.items():
    print(f"{key:9s}: {value.numpy()}")
    
    
# It should look like this:
# idx      : 1680
# label    : 0
# sentence1: b'Text text.'
# sentence2: b'Text text .'


# --------------------------------------------------


# --------------------------------------------------

## HuggingFace Datasets
# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------

# Dataset 1: for sentence translation (English to/from French)

## B-L-D: (Breakfast - Lunch - Dinner) to/from PD-D-D (Petit Dejeuner - Dejeuner - Diner)
# --------------------------------------------------

wiki_wiki = wikipediaapi.Wikipedia('MyProjectName', 'fr',
        extract_format=wikipediaapi.ExtractFormat.WIKI)

p_wiki = wiki_wiki.page("Diner")
breafast_text = p_wiki.text
print('diner_text: ', breafast_text)

# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------


# --------------------------------------------------
